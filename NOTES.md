# Notes

## 10.30.2017
- Attempted to add an extra layer with 16 output neurons. Increases post-training error. Why is that happening?

## 11.01.2017
- Activation functions can affect training
    - Vanishing Gradient Problem
- Softmax and Sigmoid are slightly different
